{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrCJy2uoQUb"
      },
      "source": [
        "# Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoOSTe5I6Om6"
      },
      "outputs": [],
      "source": [
        "# Some common packages that we may need\n",
        "from pandas.plotting import scatter_matrix \n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import some common packages\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "def load_police_data():\n",
        "  crimes=pd.read_csv('./data/Crime_Data.csv')\n",
        "  return crimes\n",
        "\n",
        "def load_oridinal_violence_features():\n",
        "  ordinal_violence=pd.read_csv('./data/crime_categories.csv')\n",
        "  return ordinal_violence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9hjjV5SDs0z"
      },
      "source": [
        "Now that the data is loaded we want to look at the data and see how it is organized and what groups are being used. We can also get some insight onto if the data may be useful adn we can look and see how we may split or encode our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "CZEtJ3IB6P3a",
        "outputId": "8355d837-c7f2-481e-ced9-984708f61573"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8d1e4133a2e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_police_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcrimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d585749ed1cc>\u001b[0m in \u001b[0;36mload_police_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_police_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mcrimes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/Crime_Data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcrimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/Crime_Data.csv'"
          ]
        }
      ],
      "source": [
        "crimes = load_police_data()\n",
        "crimes.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DdYGrkM6AAu"
      },
      "outputs": [],
      "source": [
        "crimes.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoNBGaBZDqV8"
      },
      "outputs": [],
      "source": [
        "crimes.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NELH584yoQUf"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slYFckZJIpnE"
      },
      "source": [
        "## Non Numerical Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGo8CBHUESWQ"
      },
      "source": [
        "Now lets look a little closer at the non numerical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMmPDQjqEs01"
      },
      "outputs": [],
      "source": [
        "crimes[\"Offense\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F9L_pO3E3El"
      },
      "outputs": [],
      "source": [
        "crimes[\"StreetName\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9hUNNInE-YH"
      },
      "outputs": [],
      "source": [
        "crimes[\"Agency\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkOLS_eyFEyt"
      },
      "outputs": [],
      "source": [
        "crimes[\"DateReported\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCClWbbeFIAY"
      },
      "outputs": [],
      "source": [
        "crimes[\"ReportingOfficer\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV36HCFbFMUu"
      },
      "source": [
        "There are a lot of categories in the non-numerical data. This is not as worrying as it could be since a lot of the data seems like it is unnecessary for our project. It does not seem like we are going to need Reporting Officer, Agency, and Date Reported. We can just use the hours integer to get the time. \n",
        "\n",
        "Small side note: It would be interesting to look up odds of getting arrested on certain streets depending on who is on duty, but that is outside of the scope of our particular project. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOIhssAIjpI"
      },
      "source": [
        "## Numerical Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ot4zJZOGOp9"
      },
      "source": [
        "Now we will look at histograms of the numerical data below. From this data we will determine what is important information and what is not. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2F2rp1KF7X_"
      },
      "outputs": [],
      "source": [
        "crimes.hist(bins=50, figsize=(20,15))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ByLgGyUoQUj"
      },
      "outputs": [],
      "source": [
        "crimes['HourReported'].hist(bins=24) # if we use more than 24 bins, the fact that nothing can be reported after 59 minutes makes the bar plot really weird"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeJyrfFpGcOU"
      },
      "source": [
        "The histrogram of the Record ID behaved exactly as expected. We assumed that Incident ID would also not be very useful but it is organized by year, which will not really fit into what we are doing for this part of the project, but it would be interesting to look at what crimes have become less frequent over time, and it may actually play a role so even though we initially plan on getting rid of it, we will reevaluate as we continue to work on the project. \n",
        "There are some correlations with lower block numbers having more incidents which we did not suspect as a group but more incidents at later hours was something we expected. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfQ1oFS7JF_5"
      },
      "source": [
        "## Null Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rkgN3mXrxd"
      },
      "source": [
        "First we will observe the null data and think of a plan to deal with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3FejjABJL_0"
      },
      "outputs": [],
      "source": [
        "incomplete_data = crimes[crimes.isnull().any(axis=1)].head()\n",
        "incomplete_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhkNtVQwJgbe"
      },
      "outputs": [],
      "source": [
        "crimes_without_null = crimes.dropna()\n",
        "crimes_without_null.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnVLbWixK9qM"
      },
      "outputs": [],
      "source": [
        "crimes.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv-CCkMzLC-u"
      },
      "source": [
        "Looking at the data we see that there are about 1300 missing block numbers, I am not sure how cobining this data with the streetnames is going to work but we will try to use an imputer but it may cause some strange behaviors that will need to be looked at later depending on our observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpZGV5jDKeQx"
      },
      "outputs": [],
      "source": [
        "imputer = SimpleImputer(strategy=\"median\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7M24p3RXze3"
      },
      "source": [
        "Here we are going to look at imputing the data but we decided we would get rid of the values that do not matter first. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ1e3FV8ZlxU"
      },
      "outputs": [],
      "source": [
        "crimes.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq8mWEMUZWsy"
      },
      "outputs": [],
      "source": [
        "crimes_lean = crimes.drop([\"ReportingOfficer\",\"IncidentID\",\"Agency\",\"RecordID\",],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uk_GRGlwaSrA"
      },
      "outputs": [],
      "source": [
        "crime_num = crimes_lean.drop([\"Offense\",\"DateReported\",\"StreetName\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p11irr-a1V-"
      },
      "outputs": [],
      "source": [
        "imputer.fit(crime_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgaRTCFBcCmD"
      },
      "source": [
        "Just want to look at the data in the imputed state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbZrDBBga9Dk"
      },
      "outputs": [],
      "source": [
        "imputer.statistics_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzOp0Jv7bH85"
      },
      "outputs": [],
      "source": [
        "crime_num.median().values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRRa2TLqcIYU"
      },
      "source": [
        "Now we need to deal with the categorical data. This is where some big adjustments to what we have input need to happen. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEuRvYR4oQUm"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XECS58oKoQUm"
      },
      "source": [
        "First, we are going to drop the features we determined are not particularly useful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEbh3goBoQUm"
      },
      "outputs": [],
      "source": [
        "crimes_lean = crimes.drop([\"ReportingOfficer\",\"IncidentID\",\"Agency\",\"RecordID\",],axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeDjSBKVoQUm"
      },
      "source": [
        "## Offense Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gY23EomcO_i"
      },
      "outputs": [],
      "source": [
        "crime_offense_cat = crimes[[\"Offense\"]]\n",
        "crime_offense_cat.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EutE4nGoQUm"
      },
      "source": [
        "We are going to have to label these by hand by the look of it. There's no way to infer how dangerous an offense is by machine, unless perhaps by some sophisticated NLP algorithm which would take longer to write and design than it would to do this project most likely.\n",
        "\n",
        "It is important to acknowledge the fact that our labelling of the data is an inherent source of bias. Unfortunately, if we wish to use this data for predicting safety, it will be necessary as a trade-off, because it would just be too innacurrate to base a decision on walking safety on occurence of trafic law violations or something of that nature. Thus, this is a tradeoff that has to be acknowledged as an inherent source of bias since this is not really a clear-cut line between \"violent\" and \"non-violent\". Additionally, there seem to be some categories that we may or may not actually be able to interpret clearly, such as `'Tactical Crime Initiative - TCI'`, which may mean something to the police department, but has litte meaning for us, unless it's something we can find on Google. There may be very few features with these categorizations however, so this may be a case where we can simply drop those records."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fn7wRUroQUm"
      },
      "source": [
        ". . .  time elapses . . .\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Having now hand-jammed an ordinal encoding for the categorical offense data, we can load it and see if we can't use this engineered feature to our advantage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqz2fXc5oQUm"
      },
      "outputs": [],
      "source": [
        "ord_offenses = load_oridinal_violence_features()\n",
        "ord_offenses.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3rERrqDoQUm"
      },
      "source": [
        "Unfortunately, this kind of solution is very prone to error, and introduces much inherent bias. This is probably something that it would be good to ask the police department to do, since they actually understand what all of the offences are and how dangerous they really are. Sadly, we are not the police force and they probably would not have time for such a thing if we were to ask them to do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGIPM0SMoQUn"
      },
      "source": [
        "The next task is to try and get this data mapped into the main data set. Notably, `ord_offenses` only has one entry for each kind of offense, so we will have to use some kind of pandas trick here. Or maybe just a good old fashioned nested for loop . . . Parallelization! I blow my nose at you!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiul9udqoQUn"
      },
      "outputs": [],
      "source": [
        "# first, we'll add a duplicate of the `Offense` column for the `danger` feature\n",
        "crimes_lean['Danger'] = crimes_lean.loc[:, 'Offense']\n",
        "\n",
        "# next, we'll turn the `ord_offenses` data frame into a dictionary\n",
        "danger_dict = dict(zip(ord_offenses.Offense, ord_offenses.Danger_Level))\n",
        "# print(danger_dict)\n",
        "\n",
        "# finally, we can map the new `Danger` column to it's categorical value\n",
        "crimes_lean['Danger'] = crimes_lean['Danger'].map(danger_dict).map(lambda x: int(x >= 3))\n",
        "\n",
        "# now, we can take a peek at our lovely new column!\n",
        "crimes_lean.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuCYPRs9oQUn"
      },
      "outputs": [],
      "source": [
        "crimes_lean['Danger'].hist(bins=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bfwozzxoQUn"
      },
      "source": [
        "## Date Conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhru4_kIoQUn"
      },
      "source": [
        "Another problem we need to deal with is converting the date to some sort of usable numerical format, such as milliseconds since the last unix epoch. Let's examine this feature again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6SUf8wzoQUn"
      },
      "outputs": [],
      "source": [
        "crimes_lean['DateReported'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8N_5A9RoQUo"
      },
      "source": [
        " Commence string munging tricks . . . or maybe we'll just find a library like [`dateutil`](https://labix.org/python-dateutil#head-a23e8ae0a661d77b89dfb3476f85b26f0b30349c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH8-LhxIoQUo"
      },
      "source": [
        "Yeah, the library is totally the way to go. Of course.\n",
        "\n",
        "Now, just a matter of using a map or something like that to convert the thing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzA4lqx6oQUo"
      },
      "outputs": [],
      "source": [
        "import dateutil as du\n",
        "import datetime as dt\n",
        "\n",
        "# let's start by making a copy of the column like before\n",
        "crimes_lean['Epoch'] = crimes_lean.loc[:, 'DateReported']\n",
        "\n",
        "# next, we need a function that can be mapped over the column to do the conversion.\n",
        "def date_converter(in_str):\n",
        "    date = du.parser.parse(in_str)\n",
        "    return date\n",
        "\n",
        "# finally, we can map the function across the duplicate column to get the unix epoch\n",
        "# crimes_lean['Epoch'] = crimes_lean['Epoch'].map(date_converter)\n",
        "crimes_lean['Weekday'] = crimes_lean['Epoch'].map(lambda x: date_converter(x).weekday())\n",
        "crimes_lean['Year'] = crimes_lean['Epoch'].map(lambda x: date_converter(x).year)\n",
        "crimes_lean['Month'] = crimes_lean['Epoch'].map(lambda x: date_converter(x).month)\n",
        "crimes_lean['Day'] = crimes_lean['Epoch'].map(lambda x: date_converter(x).day)\n",
        "\n",
        "# now, we can take a peek at our lovely new column!\n",
        "crimes_lean.head()\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tce8U-wBoQUo"
      },
      "source": [
        "Booyah!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrOP9Z0KoQUo"
      },
      "source": [
        "Now we can actually look at these times numerically to see trends with time for example!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fpgn7xoQUo"
      },
      "source": [
        "## Location Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5lxVYfqoQUo"
      },
      "source": [
        "### Road Centerlines Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjEjSvrjoQUo"
      },
      "source": [
        "The first step to this process is going to be to find some sort of way to translate the block system used by the Charlottesville Police Department to something that is practically useful. To this end, we have downloaded and will utilize the [Road Centerlines](https://opendata.charlottesville.org/datasets/charlottesville::road-centerlines-block-level/about) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnGBOSvtoQUs"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "import geopandas as gpd                         # this allows us to use standard geospatial formats (such as the geojson that we downloaded)\n",
        "\n",
        "road_lines_full = gpd.read_file(\"./data/Road_Centerlines_(Block_Level).geojson\")             # Note that `road_lines` is a geodataframe, and is not json-esque\n",
        "print(road_lines_full.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyPGpQeMoQUs"
      },
      "source": [
        "Oh, dear. I thought these were supposed to be centerlines . . . It looks like these are linestrings that describe the left and right edges of the road. \\*sigh\\*\n",
        "\n",
        "I suppose we could just pretend one side of the road doesn't exist . . ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcmvQGxaoQUs"
      },
      "source": [
        "It could also be that those with a L / R  attribute are for split highways with a median in between. Regardless, we don't have that level of detail in our crimes data if I am not mistaken."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsISLQqyoQUs"
      },
      "source": [
        "One thing we can try to simplify this scheme is to use the observation that the finest level of detail we have in the crimes data set is by the block number and street name. Therefore, we are not losing any precision if we were to take the center point of each block, and use that as the coordinate for any crime on that block. To do this, we can take the linestrings formed by the left and right halves of the data, draw additional lines between their starts and ends, which will turn each block into a polygon. We can then find the centerpoint of the polygon to determine a single lat-lon point for the block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTxsvj3oQUs"
      },
      "source": [
        "However, because time is short and `geopandas` is complex, we are instead just going to keep only the left side of each block, and we will use the center point of this line string as the geometric coordinate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoRST081oQUs"
      },
      "outputs": [],
      "source": [
        "# we would like to make sure that there isn't anything besides left and right\n",
        "print(road_lines_full.info())\n",
        "print(road_lines_full.value_counts([\"Side\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPNgAbmoQUt"
      },
      "source": [
        "Notably, there are more left-hand sides than right hand sides. I am guessing/assuming this is because one-way streets only have a left-hand side. This doesn't really make sense though, because we really only drive on the right-hand side. Regardless, dropping all of the right-hand rows will work for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atfdxi2doQUt"
      },
      "outputs": [],
      "source": [
        "# we start by dropping all of the right hand rows.\n",
        "\n",
        "road_lines_naive = road_lines_full[road_lines_full.Side != \"R\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqRGptOKoQUt"
      },
      "source": [
        "The center point can then be found with the this [function](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoSeries.centroid.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FX-mp8poQUt"
      },
      "outputs": [],
      "source": [
        "# we can start by making a copy of the geometry column\n",
        "road_lines_naive['point'] = road_lines_naive.loc[:, 'geometry']\n",
        "\n",
        "# then we can make our mapping function\n",
        "def naive_center_finder(linestring):\n",
        "    return gpd.GeoSeries([linestring]).centroid[0]\n",
        "\n",
        "# and then we can map our handy-dandy function over this column\n",
        "road_lines_naive['point'] = road_lines_naive['point'].map(naive_center_finder)\n",
        "\n",
        "# and finally see if it worked\n",
        "print(road_lines_naive.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlLyJibKoQUt"
      },
      "source": [
        "weeeellll . . . I suppose it worked. We have no real way of knowing whether those are actually the centerpoints I would hope to see, but hey, at least it's something."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9BMoSL6oQUt"
      },
      "source": [
        "### Using the Road Center Points in the Crimes Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxtkj6l_oQUt"
      },
      "source": [
        "To actually use what we just made, we can now use this data set to add a coordinate to each crime!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQjRXAhoQUt"
      },
      "outputs": [],
      "source": [
        "crimes_lean = crimes_lean.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN8YjWLNoQUu"
      },
      "outputs": [],
      "source": [
        "# We are going to start by concatenating the block number and street name into one field.\n",
        "# hopefully these will be identical for both data sets.\n",
        "crimes_lean['road_location'] = crimes_lean['StreetName'] + crimes_lean['BlockNumber'].astype(int).astype(str)\n",
        "# print(crimes_lean.head())\n",
        "\n",
        "road_lines_naive['road_location'] = road_lines_naive['Street'] + road_lines_naive['Block'].astype(int).astype(str)\n",
        "print(road_lines_naive.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGJmZpx0oQUu"
      },
      "outputs": [],
      "source": [
        "# next we can make a dictionary out of the road_lines_naive data set\n",
        "road_centers_dict = dict(zip(road_lines_naive.road_location, road_lines_naive.point))\n",
        "\n",
        "# and then make a point column for the crimes data\n",
        "crimes_lean['Point'] = crimes_lean.loc[:, 'road_location']\n",
        "\n",
        "# and finally we can use the dictionary to map over this column to get our points\n",
        "crimes_lean['Point'] = crimes_lean['Point'].map(road_centers_dict)\n",
        "\n",
        "print(crimes_lean.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501YVrS6oQUu"
      },
      "source": [
        "We have successfully given each crime a lat-lon coordinate, which is super fantastic! Even if we had to lose about 6k crimes in the process, and even if the point isn't as accurate as it could be since we only used half of the street."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtG6xqEuoQUu"
      },
      "source": [
        "Also, we really need that point to be split into latitude and longitude columns practically speaking . . ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiXU4qN7oQUu"
      },
      "outputs": [],
      "source": [
        "# We can do that very easily using geopandas as well\n",
        "crimes_lean = crimes_lean.dropna()\n",
        "\n",
        "crimes_lean['Longitude'] = crimes_lean['Point'].apply(lambda point: point.x)\n",
        "crimes_lean['Latitude'] = crimes_lean['Point'].apply(lambda point: point.y)\n",
        "\n",
        "print(crimes_lean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C06pKLj8oQUu"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vboGB5zcoQUu"
      },
      "outputs": [],
      "source": [
        "# and now, lets grab just the really useful features we have constructed and see if we get some interesting regression results out of it\n",
        "crimes_sparse = crimes_lean[['Weekday', 'Year', 'Month', 'Day', 'Latitude', 'Longitude', 'Danger']]\n",
        "\n",
        "print(crimes_sparse.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe2_JhF1oQUu"
      },
      "source": [
        "Wow, we say \"sparse\", but we should probably say \"Charlie-Brown-Christmas-Tree-Esque\".\n",
        "\n",
        "![charlie-brown-tree](https://wallpapercave.com/wp/9Dvub0b.jpg)\n",
        "\n",
        "It's funny how after extracting the essential information out of the data, we really have very, very little to work with for each record. Just when the reported happened, where the reported incident happend, and how bad the incident was on a scale of 0 to 4. Hopefully that's enough to do some reasonable machine learning . . ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV70dSG_oQUu"
      },
      "source": [
        "### Visualization Detour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_XxwgERoQUu"
      },
      "source": [
        "Let's do a quick 3D graph with a random subset of our data to try and get a feel for whether we can legitimately try to use a linear regression to solve our problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "RhJcaG-yoQUu",
        "outputId": "2d91b66f-7362-47ae-e234-3c9549ea3b9c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bfb470c6a06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample_crimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrimes_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'3d'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_crimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Longitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_crimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_crimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Day'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_crimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Danger'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'crimes_sparse' is not defined"
          ]
        }
      ],
      "source": [
        "sample_crimes = crimes_sparse.sample(frac=0.1, random_state=42)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = plt.axes(projection='3d')\n",
        "ax.scatter3D(sample_crimes['Longitude'], sample_crimes['Latitude'], sample_crimes['Day'], c=sample_crimes['Danger'], cmap='jet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQr_SFCZoQUv"
      },
      "source": [
        "In the above plot, the axes are longitude, latitude, and time for x, y, and z respectively, and the color of each dot is based on its danger level from 0 to 4. Although this isn't the greatest plot, I tend to think this data is _not_ going to be linearly separable by any stretch of the imagination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wkj3ZH_oQUv"
      },
      "source": [
        "So the question then becomes, if we can't do a linear regression, what should we do instead? We have just graphed the full dimensionality of our space, so SVM may not be sufficient here as there doesn't seem to be any sort of clear pattern whatsoever. SVM might still be worth looking into however, especially since it can be used to map the feature space to higher dimensions.\n",
        "\n",
        "An interesting thing to consider here is that the feature we would want to predict, `Danger`, can be treated as either a categorical or a numeric value. What this means practically is that we can treat this problem not so much as a regression, but rather as a classification problem. Unfortunately, K-Means and other clustering methods will also probably fail to be of use here since the dimensionality is so very low, although they may still be worth trying. \n",
        "\n",
        "This leaves us with Neural Nets. A deep neural net may be able to make sense of this low dimensionality feature space in a meaningful way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MrgusFVoQUv"
      },
      "source": [
        "### Neural Net mk. I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw0WkQTMoQUv"
      },
      "source": [
        "Let's give the neural net a go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxl4lyV2oQUv"
      },
      "source": [
        "We can make a validation set and a training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MluKpJdWoQUv"
      },
      "outputs": [],
      "source": [
        "# and we can also standard scale our features\n",
        "from sklearn import pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# pipeline to standard scale the input\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler())]\n",
        ")\n",
        "\n",
        "categorical_features = ['Weekday', 'Year', 'Month', 'Day']\n",
        "numeric_features = ['Latitude', 'Longitude']\n",
        "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# # full pipeline:\n",
        "# full_pipeline = ColumnTransformer([\n",
        "#         (\"num\", num_pipeline, num_attribs),\n",
        "#     ])\n",
        "nn_1_pipeline = preprocessor\n",
        "# to_be_X = crimes_sparse.drop(\"Danger\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcnaso0coQUv",
        "outputId": "fdd7731c-523d-40c0-b253-e29b2e4d10e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(12448, 58)\n",
            "(1384, 58)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = crimes_sparse.drop('Danger', axis=1)\n",
        "Y = crimes_sparse['Danger']\n",
        "\n",
        "nn_1_train_full_X, nn_1_test_X, nn_1_train_full_Y, nn_1_test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "nn_1_train_full_X = nn_1_pipeline.fit_transform(nn_1_train_full_X, nn_1_train_full_Y)\n",
        "nn_1_test_X = nn_1_pipeline.fit(nn_1_test_X, nn_1_test_Y)\n",
        "nn_1_train_x, nn_1_val_x, nn_1_train_y, nn_1_val_y = train_test_split(nn_1_train_full_X, nn_1_train_full_Y, test_size=0.1, random_state=42)\n",
        "\n",
        "print(nn_1_train_x.shape)\n",
        "\n",
        "print(nn_1_val_x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjJTVeTPoQUv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "nn_1 = tf.keras.models.Sequential(layers=[\n",
        "    keras.layers.Flatten(),        # input layer\n",
        "    keras.layers.Dense(units=100, activation='relu'),  # hidden layer\n",
        "    # keras.layers.Dense(units=1750, activation='relu'), # hidden layer\n",
        "    keras.layers.Dense(units=100, activation='relu'), # hidden layer\n",
        "    # keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=2, activation='softmax') # output softmax layer\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "erWw3Rr_oQUw",
        "outputId": "8ae647f4-ca8f-434f-f17b-f83951a2d768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "389/389 [==============================] - 3s 3ms/step - loss: 0.6495 - accuracy: 0.6445 - val_loss: 0.6448 - val_accuracy: 0.6575\n",
            "Epoch 2/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6466 - val_loss: 0.6428 - val_accuracy: 0.6582\n",
            "Epoch 3/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6504 - val_loss: 0.6452 - val_accuracy: 0.6568\n",
            "Epoch 4/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6554 - val_loss: 0.6436 - val_accuracy: 0.6539\n",
            "Epoch 5/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6663 - val_loss: 0.6499 - val_accuracy: 0.6517\n",
            "Epoch 6/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.6066 - accuracy: 0.6735 - val_loss: 0.6551 - val_accuracy: 0.6329\n",
            "Epoch 7/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5969 - accuracy: 0.6838 - val_loss: 0.6590 - val_accuracy: 0.6236\n",
            "Epoch 8/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5854 - accuracy: 0.6937 - val_loss: 0.6671 - val_accuracy: 0.6077\n",
            "Epoch 9/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5743 - accuracy: 0.7006 - val_loss: 0.6691 - val_accuracy: 0.6236\n",
            "Epoch 10/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5619 - accuracy: 0.7109 - val_loss: 0.6863 - val_accuracy: 0.6329\n",
            "Epoch 11/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5513 - accuracy: 0.7201 - val_loss: 0.6983 - val_accuracy: 0.6236\n",
            "Epoch 12/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5406 - accuracy: 0.7263 - val_loss: 0.7064 - val_accuracy: 0.6236\n",
            "Epoch 13/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5299 - accuracy: 0.7361 - val_loss: 0.7082 - val_accuracy: 0.6149\n",
            "Epoch 14/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5198 - accuracy: 0.7407 - val_loss: 0.7374 - val_accuracy: 0.5983\n",
            "Epoch 15/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7476 - val_loss: 0.7504 - val_accuracy: 0.5788\n",
            "Epoch 16/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.5011 - accuracy: 0.7529 - val_loss: 0.7537 - val_accuracy: 0.6019\n",
            "Epoch 17/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4909 - accuracy: 0.7619 - val_loss: 0.7631 - val_accuracy: 0.5968\n",
            "Epoch 18/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4822 - accuracy: 0.7634 - val_loss: 0.7817 - val_accuracy: 0.5802\n",
            "Epoch 19/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4733 - accuracy: 0.7719 - val_loss: 0.7693 - val_accuracy: 0.6026\n",
            "Epoch 20/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4660 - accuracy: 0.7750 - val_loss: 0.7956 - val_accuracy: 0.5961\n",
            "Epoch 21/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4579 - accuracy: 0.7825 - val_loss: 0.7969 - val_accuracy: 0.6026\n",
            "Epoch 22/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4487 - accuracy: 0.7849 - val_loss: 0.8251 - val_accuracy: 0.5990\n",
            "Epoch 23/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4402 - accuracy: 0.7898 - val_loss: 0.8252 - val_accuracy: 0.5932\n",
            "Epoch 24/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4321 - accuracy: 0.7951 - val_loss: 0.8411 - val_accuracy: 0.5990\n",
            "Epoch 25/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4233 - accuracy: 0.8007 - val_loss: 0.8490 - val_accuracy: 0.5889\n",
            "Epoch 26/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4174 - accuracy: 0.8022 - val_loss: 0.8659 - val_accuracy: 0.5947\n",
            "Epoch 27/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4128 - accuracy: 0.8073 - val_loss: 0.8808 - val_accuracy: 0.5896\n",
            "Epoch 28/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.4055 - accuracy: 0.8078 - val_loss: 0.8892 - val_accuracy: 0.5918\n",
            "Epoch 29/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3990 - accuracy: 0.8129 - val_loss: 0.9088 - val_accuracy: 0.5766\n",
            "Epoch 30/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3913 - accuracy: 0.8183 - val_loss: 0.9266 - val_accuracy: 0.5845\n",
            "Epoch 31/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3844 - accuracy: 0.8228 - val_loss: 0.9296 - val_accuracy: 0.5983\n",
            "Epoch 32/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3793 - accuracy: 0.8240 - val_loss: 0.9382 - val_accuracy: 0.5867\n",
            "Epoch 33/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3734 - accuracy: 0.8274 - val_loss: 0.9583 - val_accuracy: 0.5983\n",
            "Epoch 34/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3664 - accuracy: 0.8326 - val_loss: 0.9747 - val_accuracy: 0.5997\n",
            "Epoch 35/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3622 - accuracy: 0.8339 - val_loss: 1.0085 - val_accuracy: 0.5961\n",
            "Epoch 36/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3560 - accuracy: 0.8364 - val_loss: 0.9987 - val_accuracy: 0.5925\n",
            "Epoch 37/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3501 - accuracy: 0.8387 - val_loss: 1.0322 - val_accuracy: 0.5751\n",
            "Epoch 38/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3430 - accuracy: 0.8437 - val_loss: 1.0349 - val_accuracy: 0.5882\n",
            "Epoch 39/50\n",
            "389/389 [==============================] - 1s 4ms/step - loss: 0.3395 - accuracy: 0.8447 - val_loss: 1.0390 - val_accuracy: 0.5975\n",
            "Epoch 40/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3348 - accuracy: 0.8455 - val_loss: 1.0704 - val_accuracy: 0.5809\n",
            "Epoch 41/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3287 - accuracy: 0.8492 - val_loss: 1.0931 - val_accuracy: 0.5751\n",
            "Epoch 42/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3270 - accuracy: 0.8493 - val_loss: 1.0921 - val_accuracy: 0.5896\n",
            "Epoch 43/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3215 - accuracy: 0.8519 - val_loss: 1.1035 - val_accuracy: 0.5961\n",
            "Epoch 44/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3162 - accuracy: 0.8563 - val_loss: 1.1131 - val_accuracy: 0.5802\n",
            "Epoch 45/50\n",
            "389/389 [==============================] - 1s 3ms/step - loss: 0.3101 - accuracy: 0.8589 - val_loss: 1.1698 - val_accuracy: 0.5867\n",
            "Epoch 46/50\n",
            "254/389 [==================>...........] - ETA: 0s - loss: 0.2903 - accuracy: 0.8734"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-eedb5cd69e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn_1_mtrx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnn_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnn_1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_1_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_1_mtrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_1_train_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_1_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_1_val_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_1_val_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_1_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1860\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1861\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    498\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nn_1_epochs = 50 \n",
        "# nn_1_opt = tf.keras.optimizers.SGD(learning_rate=0.2, momentum=0.1)\n",
        "nn_1_opt = tf.keras.optimizers.Nadam()\n",
        "nn_1_loss = [\"sparse_categorical_crossentropy\"]\n",
        "nn_1_mtrx = [\"accuracy\"] \n",
        "nn_1.compile(loss= nn_1_loss, optimizer = nn_1_opt, metrics = nn_1_mtrx)\n",
        "history = nn_1.fit(nn_1_train_x.toarray(), nn_1_train_y, validation_data=(nn_1_val_x.toarray(), nn_1_val_y), epochs = nn_1_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgwm6NhJoQUw"
      },
      "outputs": [],
      "source": [
        "nn_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DRptDhooQUw"
      },
      "source": [
        "### SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkyR1Gx2oQUw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "from scipy.stats import reciprocal, uniform, randint\n",
        "\n",
        "\n",
        "X_train = nn_1_train_full_X.toarray()\n",
        "y_train = nn_1_train_full_Y\n",
        "X_test  = nn_1_val_x.toarray()\n",
        "y_test  = nn_1_val_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2xzRPLzoQUw",
        "outputId": "e164ac84-43a5-4b66-bad7-94980910ff54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix: [[  0 474]\n",
            " [  0 910]]\n",
            "Precision Score: 0.6575144508670521\n",
            "Recall Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "# linear\n",
        "svm_linear = SVC(kernel=\"linear\", C=1.0)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# print confusion matrix, precision score, and recall score\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f'Confusion Matrix: {confusion}')\n",
        "print(f'Precision Score: {precision}')\n",
        "print(f'Recall Score: {recall}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRfe8OnRoQUw",
        "outputId": "6e84e631-a66a-488a-a211-b3ba686eb53c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC(kernel='poly')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Polynomial\n",
        "svm_poly = SVC(kernel=\"poly\", C=1.0)\n",
        "svm_poly.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1NEc8KCoQUy",
        "outputId": "32ee5d6b-fe2b-47db-ae38-4c95980686ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC()"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Gaussian RBF \n",
        "svm_rbf = SVC(kernel=\"rbf\", C=1.0)\n",
        "svm_rbf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARXciNvgoQUy",
        "outputId": "3fa05858-cadd-4921-96d8-ed343131c4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
            "[CV 1/3] END C=19.34347898661638, gamma=0.03625617634576227;, score=0.649 total time=  25.1s\n",
            "[CV 2/3] END C=19.34347898661638, gamma=0.03625617634576227;, score=0.649 total time=  23.5s\n",
            "[CV 3/3] END C=19.34347898661638, gamma=0.03625617634576227;, score=0.649 total time= 1.3min\n"
          ]
        }
      ],
      "source": [
        "classifiers = [svm_linear, svm_poly, svm_rbf]\n",
        "names = ['linear', 'poly', 'rbf']\n",
        "\n",
        "param_distributions = [{\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 100)},\n",
        "                       {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 100), \"degree\": randint(2, 10)},\n",
        "                       {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 100)}]\n",
        "\n",
        "for i in [0, 1, 2]:\n",
        "  rnd_search_cv = RandomizedSearchCV(classifiers[i], param_distributions[i], n_iter=5, verbose=3, cv=3)\n",
        "  rnd_search_cv.fit(X_train, y_train)\n",
        "  print('\\n\\n-----------------------------------------------------------------')\n",
        "  print(f'svc with {names[i]} kernel:')\n",
        "  print(f'best score: {rnd_search_cv.best_score_}')\n",
        "  print(f'best_estimator: {rnd_search_cv.best_estimator_}')\n",
        "\n",
        "  # print confusion matrix, precision score, and recall score\n",
        "  y_pred = rnd_search_cv.predict(X_test)\n",
        "  confusion = confusion_matrix(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  print(f'Confusion Matrix: {confusion}')\n",
        "  print(f'Precision Score: {precision}')\n",
        "  print(f'Recall Score: {recall}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQA17zFiyCpa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AIOIhssAIjpI",
        "yfQ1oFS7JF_5",
        "WeDjSBKVoQUm",
        "5bfwozzxoQUn",
        "x-fpgn7xoQUo",
        "k5lxVYfqoQUo",
        "H9BMoSL6oQUt",
        "2MrgusFVoQUv",
        "5DRptDhooQUw"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}